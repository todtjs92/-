{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEDscraper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T06:56:51.108234Z",
     "start_time": "2020-02-24T06:56:47.569217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from requests import request\n",
    "from requests.compat import urljoin, urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from requests import Session\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import simplejson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀레늄이용해서 긁어올 동영상 링크 리스트로만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
    "url='https://www.ted.com/talks?language=en&page=1&sort=newest'\n",
    "params={'language':'en','page':1,'sort':'newest'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_path='C:/Users/todtj/OneDrive/바탕 화면/jungeui_git/voice_conversion_and_evlauation/crawling_ted_audio_files_scripts/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
    "url='https://www.ted.com/talks?language=en&page=1&sort=newest'\n",
    "params={'language':'en','page':1,'sort':'newest'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url) # url 받아오기\n",
    "video=list()  # video에 \n",
    "for _ in driver.find_elements_by_class_name('talk-link'):\n",
    "   video.append(_.find_element_by_css_selector('a').get_attribute('href'))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## Soup Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:30.434915Z",
     "start_time": "2020-04-24T07:19:30.420388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SoupMaker:\n",
    "    def sleep_short(self):\n",
    "        return time.sleep(random.uniform(0, .2))\n",
    "    def sleep_two(self):\n",
    "        return time.sleep(random.uniform(.5, 2))\n",
    "    def sleep_five(self):\n",
    "        return time.sleep(random.uniform(3, 5))\n",
    "    def make_soup(self, url):\n",
    "        # user_agent 랜덤하게 만들기\n",
    "        user_agent = {'User-agent': UserAgent().random}\n",
    "        # request page and make soup\n",
    "        page = requests.get(url, headers=user_agent)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        return soup\n",
    "    def taste_soup(self, soup):\n",
    "        try:\n",
    "            taster = soup.title.text\n",
    "            bad_soup = re.search(r'404: Not Found', taster)\n",
    "        except AttributeError:\n",
    "            bad_soup = None\n",
    "        return bad_soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CreateCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동영상제목 ,키값, url, 동영상길이등 정보담은 csv파일 만들기\n",
    "class CreateCSV(SoupMaker):\n",
    "    def create_topics_csv(self):\n",
    "        soup = self.make_soup('https://www.ted.com/topics')\n",
    "        topic_list = []\n",
    "        topic_tag = soup.find_all(class_='d:b', style='line-height:3;')\n",
    "        for tag in topic_tag:\n",
    "            topic = re.sub(r'\\s+', '', tag.text)\n",
    "            topic_list.append(topic)\n",
    "        topics_series = pd.Series(topic_list, name='Topic')\n",
    "        topics_series.to_csv('../data/topics.csv', index=False)\n",
    "    def create_languages_csv(self):\n",
    "        lang_url = 'https://www.ted.com/participate/translate/our-languages'\n",
    "        soup = self.make_soup(lang_url)\n",
    "        lang_list = []\n",
    "        lang_tags = soup.find_all('div', class_='h9')\n",
    "        for tag in lang_tags:\n",
    "            if tag.a == None:\n",
    "                continue\n",
    "            else:\n",
    "                lang_code = re.search(r'(?<=\\=)[\\w-]+', tag.a['href']).group(0)\n",
    "                lang_name = tag.text\n",
    "                lang_list.append([lang_code] + [lang_name])\n",
    "        lang_df = pd.DataFrame(data=lang_list, columns=['lang_code', 'language'])\n",
    "        lang_df.to_csv('../data/languages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Talk Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:35.650462Z",
     "start_time": "2020-04-24T07:19:35.604835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# csv파일에 들어갈 column 값들 정하기\n",
    "class TalkFeatures(SoupMaker):\n",
    "    def get_talk_id(self, soup):\n",
    "        talk_id = re.search('meta content=\"ted://talks/(\\d+)',str(soup))\n",
    "        return talk_id.group(1)\n",
    "    def get_title(self, soup):\n",
    "        title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "        tag_list = title_tag.split(':')\n",
    "        title = \":\".join(tag_list[1:]).lstrip()\n",
    "        return title\n",
    "    def get_speaker_1(self, soup):\n",
    "        try:\n",
    "            speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", str(soup))[0]\n",
    "            # convert to DataFrame\n",
    "            speakers_df = pd.read_json(speaker_tag)\n",
    "            full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                         + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                         + speakers_df.loc[:, 'lastname'])\n",
    "            full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "            # transform series to a dict\n",
    "            speaker = full_name_clean.iloc[0]\n",
    "        except:\n",
    "            speaker = re.search(r\"(?<=\\\"speaker_name\\\":)\\\"(.*?)\\\"\", str(soup)).group(1)\n",
    "        return speaker\n",
    "    def get_all_speakers(self, soup):\n",
    "        try:\n",
    "            speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", str(soup))[0]\n",
    "            # convert to DataFrame\n",
    "            speakers_df = pd.read_json(speaker_tag)\n",
    "            full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                         + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                         + speakers_df.loc[:, 'lastname'])\n",
    "            full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "            # transform series to a dict\n",
    "            speakers = full_name_clean.to_dict()\n",
    "        except:\n",
    "            speakers = None\n",
    "        return speakers\n",
    "    def get_occupations(self, soup):\n",
    "        try:\n",
    "            occupations_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", str(soup))[0]\n",
    "            # convert json to DataFrame\n",
    "            occupations_series = pd.read_json(occupations_tag)['description']\n",
    "            if occupations_series.all():\n",
    "                # clean and create dict\n",
    "                occupations = occupations_series.str.lower().str.split(', ')\n",
    "                occupations = occupations.to_dict()\n",
    "            else:\n",
    "                occupations = None\n",
    "        except:\n",
    "            occupations = None\n",
    "        return occupations\n",
    "    def get_about_speakers(self, soup):\n",
    "        try:\n",
    "            speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", str(soup))[0]\n",
    "            # convert to DataFrame\n",
    "            about_series = pd.read_json(speaker_tag)['whotheyare']\n",
    "            if about_series.all():\n",
    "                # transform series to a dict\n",
    "                about_speakers = about_series.to_dict()\n",
    "            else:\n",
    "                about_speakers = None\n",
    "        except:\n",
    "            about_speakers = None\n",
    "        return about_speakers\n",
    "    def get_views(self, soup):\n",
    "        view_count = re.search(r\"(?<=\\\"viewed_count\\\":)\\d+\", str(soup)).group(0)\n",
    "        return view_count\n",
    "\n",
    "    def get_recorded_date(self, soup):\n",
    "        try:\n",
    "            tag = re.search(r\"(?<=\\\"recorded_at\\\":\\\")[\\d-]+\", str(soup))\n",
    "            recorded_at = tag.group(0)\n",
    "        except:\n",
    "            recorded_at = None\n",
    "        return recorded_at\n",
    "\n",
    "    def get_published_date(self, soup):\n",
    "        published_raw = soup.find(attrs={'itemprop': 'uploadDate'}).attrs['content']\n",
    "        published_date = re.search(r\"[\\d-]+\", published_raw).group(0)\n",
    "        return published_date\n",
    "    def get_event(self, soup):\n",
    "        event = re.search(r\"(?<=\\\"event\\\":)\\\"(.*?)\\\"\", str(soup)).group(1)\n",
    "        return event\n",
    "    def get_native_lang(self, soup):\n",
    "        native_lang = re.search(r'(?<=nativeLanguage\\\":)\\\"(.*?)\\\"', str(soup)).group(1)\n",
    "        return native_lang\n",
    "    def get_available_lang(self, soup):\n",
    "        languages = re.findall(r'(?<=languageCode\\\":)\\\"(.*?)\\\"', str(soup))\n",
    "        clean_lang = sorted(list(set(languages)))\n",
    "        return clean_lang\n",
    "    def get_comments_count(self, soup):\n",
    "        try:\n",
    "            comments_count = re.search(r\"(?<=\\\"count\\\":)(\\d+)\", str(soup)).group(1)\n",
    "        except AttributeError:\n",
    "            comments_count = None\n",
    "        return comments_count\n",
    "    def get_duration(self, soup):\n",
    "        duration =  re.search(r\"(?<=\\\"duration\\\":)(\\d+)\", str(soup)).group(1)\n",
    "        return duration\n",
    "    def get_topics(self, soup):\n",
    "        match_obj = re.search(r\"\\\"tag\\\":\\\"(.*?)\\\"\", str(soup))\n",
    "        topics = match_obj.group(1).split(',')\n",
    "        return topics\n",
    "    def get_related_talks(self, soup):\n",
    "        related_tag = re.search(r\"(?<=\\\"related_talks\\\":).*?]\", str(soup)).group(0)\n",
    "        related_sr = pd.read_json(related_tag)\n",
    "        related_talks = dict(zip(related_sr['id'], related_sr['title']))\n",
    "        return related_talks\n",
    "    def get_talk_url(self, soup):\n",
    "        talk_tag = soup.find(attrs={'property': 'og:url'}).attrs['content']\n",
    "        talk_url = talk_tag.split('transcript')[0]\n",
    "        return talk_url\n",
    "    def get_description(self, soup):\n",
    "        desc_tag = soup.find(attrs={'property': 'og:description'}).attrs['content']\n",
    "        talk_desc = desc_tag.split(': ', 1)[1]\n",
    "        return talk_desc\n",
    "    def get_transcript(self, soup):\n",
    "        transcript = ''\n",
    "        transcript_strings = []\n",
    "        for div in soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\"):\n",
    "            for p in div.find_all('p'):\n",
    "                # add every string in the transcript to a list\n",
    "                transcript_strings.append(\" \".join(p.text.split()))\n",
    "            else:\n",
    "                # after all strings have been added, create a single transcript string\n",
    "                transcript = \" \".join(transcript_strings)\n",
    "        return transcript\n",
    "    def download_transcript_with_time(self, talk_id):\n",
    "        soup = SoupMaker()\n",
    "        script_soup = soup.make_soup(\"https://hls.ted.com/talks/\"+talk_id+\"/subtitles/ko/full.vtt\")\n",
    "        scr = str(script_soup)[70:-18]\n",
    "        script_dict={}\n",
    "        index = 0;\n",
    "        tmp = {}\n",
    "        for part in scr.split('\\n'):\n",
    "            if part.find('--&gt;')!=-1:\n",
    "                script_dict[index] = tmp.copy()\n",
    "                tmp={}\n",
    "                index += 1 \n",
    "                times = part.split('--&gt;')\n",
    "                tmp['start'] = times[0][:-1]\n",
    "                tmp['end'] = times[1][1:]\n",
    "            elif part!='':\n",
    "                try:\n",
    "                    tmp['sentence1']\n",
    "                except:\n",
    "                    tmp['sentence1'] = part\n",
    "                    continue\n",
    "                try:\n",
    "                    tmp['sentence2']\n",
    "                except:\n",
    "                    tmp['sentence2'] = part\n",
    "                    continue\n",
    "                try:\n",
    "                    tmp['sentence3']\n",
    "                except:\n",
    "                    tmp['sentence3'] = part\n",
    "        df = pd.DataFrame.from_dict(script_dict,orient='index')\n",
    "        df.to_csv('../data/transcript_'+talk_id+'.csv', index=False)\n",
    "        print(\"downloded at '../data/transcript_\"+talk_id+\".csv'\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:36.214636Z",
     "start_time": "2020-04-24T07:19:36.181907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class URLs(SoupMaker):\n",
    "    def topics_url_param(self):\n",
    "        topics_param = ''\n",
    "        if self.topics != 'all':\n",
    "            if isinstance(self.topics, list):\n",
    "                for topic in self.topics:\n",
    "                    topics_param += ('&topics[]=' + topic)\n",
    "            else:\n",
    "                raise ValueError(\"'topics' param needs to be a list\")\n",
    "        return topics_param\n",
    "    def get_max_page(self):\n",
    "        page_num = [1]\n",
    "        # make soup from ted.com/talks with specified language\n",
    "        soup = self.make_soup(self.base_url + '&page=1&sort=newest')\n",
    "        # iterate through each pagination element and get the max\n",
    "        page_elem = soup.find_all('a', class_='pagination__item pagination__link')\n",
    "        for element in page_elem:\n",
    "            page_num.append(int(element.text))\n",
    "        return max(page_num)\n",
    "    def get_all_url_paths(self):\n",
    "        url_path_list = []\n",
    "        # construct url with lang code specified by the user\n",
    "        talks_url = (self.base_url + '&page=')\n",
    "        # set range from 1 to the max page in the pagination element\n",
    "        page_range = range(1, self.get_max_page()+1)\n",
    "        # iterate through each page and get the url for each talk\n",
    "        for i in page_range:\n",
    "            # try a second attempt if first attempt fails\n",
    "            for attempt in range(2):\n",
    "                try:\n",
    "                    talks_page_url = talks_url + str(i) + '&sort=newest'\n",
    "                    soup = self.make_soup(talks_page_url)\n",
    "                    # delay between searches\n",
    "                    self.sleep_short()\n",
    "                    for div in soup.find_all('div', attrs={'class': 'media__image'}):\n",
    "                        for a in div.find_all('a'):\n",
    "                            url_path_list.append(a.get('href'))\n",
    "                except:\n",
    "                    # delay before continuing to second attempt\n",
    "                    self.sleep_two()\n",
    "                # break from attempts loop if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        return url_path_list\n",
    "    def get_all_urls(self):\n",
    "        # '/talks/jen_gunter_why_can_t_we_talk_about_periods?language=fa'\n",
    "        url_list = []\n",
    "        for url in self.get_all_url_paths():\n",
    "            url_list.append(('https://www.ted.com'\n",
    "                             + url.replace(\n",
    "                                 # to replace\n",
    "                                 '?language=' + self.lang_code,\n",
    "                                 # replace with\n",
    "                                 '/transcript' + '?language=' + self.lang_code)\n",
    "                            ))\n",
    "        return url_list\n",
    "    def clean_urls(self, urls):\n",
    "        clean_urls = []\n",
    "        for idx, url in enumerate(urls):\n",
    "            if url.startswith('https://www.ted.com/talks'):\n",
    "                parts = url.split('/')\n",
    "                joined = '/'.join(parts[:5])\n",
    "                clean = joined.split('?')\n",
    "                lang = clean[0] + '/transcript?language=' + self.lang_code\n",
    "                topic = lang + self.topics_url_param()\n",
    "                clean_urls.append(lang)\n",
    "            else:\n",
    "                print(f'bad url @ {idx} >> {url}')\n",
    "                continue\n",
    "        return clean_urls\n",
    "    def url_issues(self):\n",
    "        issues_df = pd.read_csv('../data/known_issues.csv')\n",
    "        return issues_df\n",
    "    def remove_urls_with_issues(self):\n",
    "        urls = self.all_urls()\n",
    "        final_urls = []\n",
    "        removed_urls = []\n",
    "        removed_counter = 0\n",
    "        issues_df = pd.read_csv('../data/known_issues.csv')\n",
    "        for url in urls:\n",
    "            try:\n",
    "                base_url = url.replace('transcript?language=' + self.lang_code, '')\n",
    "                # is base url in the issues df?\n",
    "                url_in_issues = (issues_df['url'] == base_url).any()\n",
    "                # get the lang_codes of the base_url\n",
    "                langs = issues_df.loc[issues_df['url'] == base_url, 'lang_code']\n",
    "                # check if the url in issues_df\n",
    "                if not url_in_issues:\n",
    "                    final_urls.append(url)\n",
    "                # if the url is in issues_df, check if it's for the same lang_code\n",
    "                elif self.lang_code in langs.any():\n",
    "                    removed_urls.append(url)\n",
    "                    removed_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    final_urls.append(url)\n",
    "            except:\n",
    "                removed_urls.append(url)\n",
    "                removed_counter += 1\n",
    "                continue\n",
    "        if removed_urls:\n",
    "            print(f\"Removed the following {removed_counter} urls as they have \"\n",
    "                  \"known issues:\\n\", removed_urls, end='\\n\\n')\n",
    "        return final_urls\n",
    "    def all_urls(self):\n",
    "        # define url attribute\n",
    "        if self.urls == 'all':\n",
    "            urls = self.get_all_urls()\n",
    "        else:\n",
    "            if isinstance(self.urls, list):\n",
    "                urls = self.clean_urls(self.urls)\n",
    "            else:\n",
    "                raise ValueError(\"'urls' param needs to be a list\")\n",
    "        return urls\n",
    "    def final_urls(self):\n",
    "        # define url attribute\n",
    "        if self.force_fetch:\n",
    "            urls = self.all_urls()\n",
    "        else:\n",
    "            urls = self.remove_urls_with_issues()  \n",
    "        return urls\n",
    "    def seen_urls(self, url, attempt):\n",
    "        if url not in self.seen:\n",
    "            yield url\n",
    "            seen.add(url)\n",
    "        # if the url was appended earlier after 2 failed attempts\n",
    "        # it means this is the last attempt (3)\n",
    "        elif url in self.seen and attempt == 1:\n",
    "            attempt = 3\n",
    "        return attempt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "## TEDscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "        lang_code (str): Language code. Defaults to 'en'.\n",
    "        language (str): Language name derived from lang_code.\n",
    "        urls (list): URLs of talks. Defaults to 'all'.\n",
    "        topics (list): Talk topics. Defaults to 'all'.\n",
    "        exclude (bool): Exclude transcript. Defaults to False.\n",
    "        ted_dict (dict): Dict to store ted talk features after scraping.\n",
    "        dict_id (int): Index of nested dict in 'ted_dict'.\n",
    "        failed_counter: Counts urls that failed to get scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T07:32:19.545188Z",
     "start_time": "2020-04-24T07:32:19.494574Z"
    }
   },
   "outputs": [],
   "source": [
    "class TEDscraper(TalkFeatures, URLs):\n",
    "    def __init__(self, lang_code='en', urls='all', topics='all',\n",
    "                 force_fetch = False, exclude_transcript=False):\n",
    "        self.lang_code = lang_code\n",
    "        self.language = self.convert_lang_code()\n",
    "        self.urls = urls\n",
    "        self.topics = topics\n",
    "        self.exclude = exclude_transcript\n",
    "        self.ted_dict = {}\n",
    "        self.dict_id = 0\n",
    "        self.failed_counter = 0\n",
    "        self.failed_urls = []\n",
    "        self.force_fetch = force_fetch\n",
    "        self.seen = set()\n",
    "        self.base_url = ('https://www.ted.com/talks'\n",
    "                         + '?language=' + self.lang_code\n",
    "                         + self.topics_url_param())\n",
    "    def scrape_all_features(self, soup):\n",
    "        # create nested dict\n",
    "        self.ted_dict[self.dict_id] = {}\n",
    "        nested_dict = self.ted_dict[self.dict_id]\n",
    "        # add the features to the nested dict\n",
    "        nested_dict['talk_id'] = self.get_talk_id(soup)\n",
    "        nested_dict['title'] = self.get_title(soup)\n",
    "        nested_dict['speaker_1'] = self.get_speaker_1(soup)\n",
    "        nested_dict['all_speakers'] = self.get_all_speakers(soup)\n",
    "        nested_dict['occupations'] = self.get_occupations(soup)\n",
    "        nested_dict['about_speakers'] = self.get_about_speakers(soup)\n",
    "        nested_dict['views'] = self.get_views(soup)\n",
    "        nested_dict['recorded_date'] = self.get_recorded_date(soup)\n",
    "        nested_dict['published_date'] = self.get_published_date(soup)\n",
    "        nested_dict['event'] = self.get_event(soup)\n",
    "        nested_dict['native_lang'] = self.get_native_lang(soup)\n",
    "        nested_dict['available_lang'] = self.get_available_lang(soup)\n",
    "        nested_dict['comments'] = self.get_comments_count(soup)\n",
    "        nested_dict['duration'] = self.get_duration(soup)\n",
    "        nested_dict['topics'] = self.get_topics(soup)\n",
    "        nested_dict['related_talks'] = self.get_related_talks(soup)\n",
    "        nested_dict['url'] = self.get_talk_url(soup)\n",
    "        nested_dict['description'] = self.get_description(soup)\n",
    "        # add transcript if param is set to False (default)\n",
    "        if not self.exclude:\n",
    "            nested_dict['transcript'] = self.get_transcript(soup)\n",
    "        return nested_dict\n",
    "    def get_data(self):\n",
    "        print(\"Fetching urls...\\n\")\n",
    "        urls = self.final_urls()\n",
    "        print(f\"Scraping {len(urls)} TED talks in '{self.language}'...\")\n",
    "        print(f\"Estimated time to complete is {round((.9*len(urls)/60), 1)} minutes\\n\")\n",
    "        # iterate through each TED talk transcript url\n",
    "        for url in urls:\n",
    "            # delay between each scrape\n",
    "            self.sleep_short()\n",
    "            # try up to three attempts\n",
    "            for i in range(1, 4):\n",
    "                # check if url has been seen, if true:\n",
    "                # it means it previously failed twice so make it the final attempt\n",
    "                attempt = self.seen_urls(url, i)\n",
    "                try:\n",
    "                    # make soup\n",
    "                    soup = self.make_soup(url)                                        \n",
    "                    # create nested dict\n",
    "                    self.ted_dict[self.dict_id] = {}\n",
    "                    # scrape features and add to a nested dict\n",
    "                    self.scrape_all_features(soup)\n",
    "                except Exception as e:\n",
    "                    # taste if it's a bad soup\n",
    "                    if self.taste_soup(soup):\n",
    "                        print(f\"[BAD_SOUP] {url}\")\n",
    "                        self.failed_urls.append(url)\n",
    "                        self.failed_counter += 1\n",
    "                        break\n",
    "                    elif attempt == 1:\n",
    "                        # 3-5 second delay before another attempt\n",
    "                        self.sleep_five()\n",
    "                        continue\n",
    "                    elif attempt == 2:\n",
    "                        # append the url to 'urls' to try again later\n",
    "                        urls.append(url)\n",
    "                        break\n",
    "                    elif attempt == 3:\n",
    "                        print(f\"[EXCEPTION] {e} {url}\")\n",
    "                        self.failed_counter += 1\n",
    "                        self.failed_urls.append(url)\n",
    "                        break\n",
    "                else:\n",
    "                    # indicate successful scrape\n",
    "                    print(f\"[OK] {self.dict_id} {url}\")\n",
    "                    # add 1 to create a new nested dict\n",
    "                    self.dict_id += 1\n",
    "                    # exit attempts loop\n",
    "                    break\n",
    "        # print results\n",
    "        print(f\"\"\"\\nTed.com scraping results:\n",
    "            \\n\\t• Successful: {self.dict_id}\n",
    "            \\n\\t• Failed: {self.failed_counter}\\n\"\"\")\n",
    "        if self.failed_counter:\n",
    "            print(f\"Failed to scrape:\\n{self.failed_urls}\\n\")\n",
    "        return self.ted_dict\n",
    "    def convert_lang_code(self):\n",
    "        df = pd.read_csv('../data/languages.csv')\n",
    "        lang_series = df.loc[(df['lang_code'] == self.lang_code), 'language']\n",
    "        language = lang_series.values[0]\n",
    "        return language\n",
    "    def to_dataframe(self, ted_dict):\n",
    "        \"\"\"Returns sorted DataFrame object from dict.\"\"\"\n",
    "        df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "        df = df.sort_values(by='published_date')\n",
    "        sorted_df = df.reset_index(drop=True)\n",
    "        return sorted_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false"
   },
   "source": [
    "## Get Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunk 단위로 자른 문장과 시간정보  csv파일로만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자막 가져오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ted_url에 받고싶은 동영상의 url 입력\n",
    "soup = SoupMaker()\n",
    "talk = TalkFeatures()\n",
    "ted_url = \"https://www.ted.com/talks/claudia_miner_a_new_way_to_get_every_child_ready_for_kindergarten/\"\n",
    "ms = soup.make_soup(ted_url)\n",
    "talk_id = talk.get_talk_id(ms)\n",
    "talk.download_transcript_with_time(talk_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동영상 정보를 담은 csv파일만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    ted_url  = df['url'][i]\n",
    "    ms = soup.make_soup(ted_url)\n",
    "    talk_id = talk.get_talk_id(ms)\n",
    "    talk.download_transcript_with_time(talk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the scraper & pass in optional arguments\n",
    "scraper = TEDscraper(lang_code='en', urls='all', topics='all')\n",
    "# scrape the data and save it to a dictionary\n",
    "ted_dict = scraper.get_data()\n",
    "# transform the dictionary to a sorted pandas DataFrame\n",
    "df = scraper.to_dataframe(ted_dict)\n",
    "# fill NoneType data into int(0)\n",
    "for i in range(0,len(df)):\n",
    "    \n",
    "    if str(type(df['comments'][i])) ==\"<class 'NoneType'>\":\n",
    "        df['comments'][i]=0\n",
    "    print(i,type(df['comments'][i]))\n",
    "    if str(type(df['views'][i])) ==\"<class 'NoneType'>\":\n",
    "        df['views'][i]=0\n",
    "# change data type\n",
    "for i in range(0,len(df)):\n",
    "    df['talk_id'][i]=int(df['talk_id'][i])\n",
    "    df['views'][i]=int(df['views'][i])\n",
    "    df['comments'][i]=int(df['comments'][i])\n",
    "    df['duration'][i]=int(df['duration'][i])\n",
    "df=df.sort_values(['talk_id'])\n",
    "# output DataFrame as CSV\n",
    "for i in range(0,len(df)):\n",
    "    df['talk_id'][i]=int(df['talk_id'][i])\n",
    "    df['views'][i]=int(df['views'][i])\n",
    "    df['comments'][i]=int(df['comments'][i])\n",
    "    df['duration'][i]=int(df['duration'][i])    \n",
    "df.to_csv('../data/ted_talks.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
